{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61dcc254-f9ba-4d03-a49a-a1a8ddbb7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, DataCollatorWithPadding\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import random\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8371cf5-24e3-4942-9418-a7efd8a53f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "predictions_file = os.path.join(results_dir, \"loan_predictions.csv\")\n",
    "metrics_file = os.path.join(results_dir, \"metrics.txt\")\n",
    "\n",
    "if not os.path.exists(predictions_file):\n",
    "    with open(predictions_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Loan_ID\", \"Loan_Data\", \"True_Label\", \"Predicted_Label\", \"Probability\"])\n",
    "    print(f\"Result file '{predictions_file}' initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2aa59d-c46a-462a-888e-9034483ea0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础模型名称和训练后的模型路径\n",
    "base_model_name = \"model/Mistral-7B-Instruct-v0.3\"\n",
    "model_path = \"outputs/mistral-7b-instruct-v0.3-0926(augmented)/checkpoint-35000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c01d0e9-2bee-4efa-9976-9b1db72c2834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from model_path.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(\"Tokenizer loaded from model_path.\")\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    print(\"Tokenizer not found in model_path, loaded from base_model_name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4095f11-53eb-464c-ad56-cf35ee5984cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer already has pad_token: [PAD]\n"
     ]
    }
   ],
   "source": [
    "# 检查并添加pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"Added pad token to tokenizer.\")\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "else:\n",
    "    print(f\"Tokenizer already has pad_token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e0b6f4-c061-4655-9ca9-6592ba7f9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b9623d-abb7-4a61-8b8e-2fa39aa6dc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8feda0c2297c4c41a306a04c5a794d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at model/Mistral-7B-Instruct-v0.3 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": device}, \n",
    "    num_labels=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c41a48-535a-44fb-9e9d-ca7cd4a06685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized model embeddings to match tokenizer vocab size: 32769\n"
     ]
    }
   ],
   "source": [
    "# 调整基础模型的嵌入层尺寸\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized model embeddings to match tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f776ec13-03a9-4a53-83c6-6d74d22b1d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter loaded.\n"
     ]
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    model_path,\n",
    "    device_map={\"\": device},\n",
    ")\n",
    "print(\"LoRA adapter loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2479e3b8-ef5e-42ec-967f-a548eca86933",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863352bd-495c-40c1-87fb-f7764dfb0814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model moved to device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4df874ee-54ac-4a90-8164-664686b985ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_path = \"example\"\n",
    "dataset = load_dataset(loan_data_path)\n",
    "val_data = dataset[\"validation\"]\n",
    "\n",
    "label_1_data = [data for data in val_data if data['label'] == 1]\n",
    "label_0_data = [data for data in val_data if data['label'] == 0]\n",
    "\n",
    "num_label_1 = len(label_1_data)\n",
    "balanced_label_0_data = random.sample(label_0_data, num_label_1)\n",
    "balanced_data = label_1_data + balanced_label_0_data\n",
    "\n",
    "# random data layout\n",
    "random.shuffle(balanced_data)\n",
    "\n",
    "dataset = Dataset.from_list(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e6f955-e331-4cac-980c-eaab9b7d6b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6a676c2c7343b3a5d58dc2e617696d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data(examples):\n",
    "    examples[\"loan_data\"] = examples.pop(\"text\")\n",
    "    examples[\"labels\"] = int(examples.pop(\"label\"))\n",
    "    return examples\n",
    "\n",
    "test_data = dataset.map(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c24cf297-1452-44de-868d-0f4d23e3ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83898e9e51b74b1bb8b0b8ee21627051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"loan_data\"],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "tokenized_test_data = test_data.map(tokenize_function, batched=True)\n",
    "tokenized_test_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bc847bb-548d-44f5-a792-effd1ed99b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataLoader created.\n"
     ]
    }
   ],
   "source": [
    "# DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_test_data,\n",
    "    batch_size=32,           # 根据显存情况调整\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,  # 使用 DataCollator 处理填充\n",
    ")\n",
    "\n",
    "print(\"Test DataLoader created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab09576-ce10-4820-8506-26d9165412e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "all_ids = []  # 存储 Loan_ID 或其他唯一标识符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec48431-900a-4c59-b211-c87e9a16198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/17 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Evaluating:   6%|▌         | 1/17 [00:02<00:45,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 0, True: 0, Pred: 0, Prob: 0.2737\n",
      "ID: 1, True: 1, Pred: 1, Prob: 0.7339\n",
      "ID: 2, True: 0, Pred: 1, Prob: 0.9336\n",
      "ID: 3, True: 0, Pred: 0, Prob: 0.4111\n",
      "ID: 4, True: 1, Pred: 0, Prob: 0.2043\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|█▏        | 2/17 [00:04<00:31,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 32, True: 1, Pred: 1, Prob: 0.9976\n",
      "ID: 33, True: 1, Pred: 1, Prob: 0.8613\n",
      "ID: 34, True: 0, Pred: 1, Prob: 0.9136\n",
      "ID: 35, True: 1, Pred: 0, Prob: 0.1571\n",
      "ID: 36, True: 1, Pred: 1, Prob: 0.7607\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  18%|█▊        | 3/17 [00:05<00:25,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 64, True: 1, Pred: 1, Prob: 0.6548\n",
      "ID: 65, True: 0, Pred: 0, Prob: 0.0368\n",
      "ID: 66, True: 0, Pred: 0, Prob: 0.3850\n",
      "ID: 67, True: 0, Pred: 1, Prob: 0.9570\n",
      "ID: 68, True: 1, Pred: 1, Prob: 0.8740\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|██▎       | 4/17 [00:07<00:22,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 96, True: 0, Pred: 0, Prob: 0.0003\n",
      "ID: 97, True: 0, Pred: 0, Prob: 0.0474\n",
      "ID: 98, True: 1, Pred: 1, Prob: 0.9663\n",
      "ID: 99, True: 0, Pred: 1, Prob: 0.7280\n",
      "ID: 100, True: 1, Pred: 0, Prob: 0.2018\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  29%|██▉       | 5/17 [00:09<00:20,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 128, True: 0, Pred: 0, Prob: 0.3557\n",
      "ID: 129, True: 0, Pred: 1, Prob: 0.9775\n",
      "ID: 130, True: 1, Pred: 0, Prob: 0.0000\n",
      "ID: 131, True: 0, Pred: 0, Prob: 0.0039\n",
      "ID: 132, True: 0, Pred: 0, Prob: 0.0100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|███▌      | 6/17 [00:10<00:17,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 160, True: 1, Pred: 1, Prob: 0.5698\n",
      "ID: 161, True: 1, Pred: 1, Prob: 0.8115\n",
      "ID: 162, True: 0, Pred: 1, Prob: 0.8232\n",
      "ID: 163, True: 0, Pred: 1, Prob: 0.7373\n",
      "ID: 164, True: 1, Pred: 0, Prob: 0.2539\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|████      | 7/17 [00:12<00:15,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 192, True: 0, Pred: 0, Prob: 0.1251\n",
      "ID: 193, True: 0, Pred: 0, Prob: 0.3557\n",
      "ID: 194, True: 1, Pred: 1, Prob: 0.6152\n",
      "ID: 195, True: 0, Pred: 0, Prob: 0.0321\n",
      "ID: 196, True: 0, Pred: 0, Prob: 0.0000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  47%|████▋     | 8/17 [00:13<00:14,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 224, True: 0, Pred: 0, Prob: 0.0863\n",
      "ID: 225, True: 1, Pred: 1, Prob: 0.5776\n",
      "ID: 226, True: 0, Pred: 0, Prob: 0.0032\n",
      "ID: 227, True: 1, Pred: 1, Prob: 0.6372\n",
      "ID: 228, True: 1, Pred: 0, Prob: 0.2583\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  53%|█████▎    | 9/17 [00:15<00:12,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 256, True: 1, Pred: 1, Prob: 0.7905\n",
      "ID: 257, True: 0, Pred: 0, Prob: 0.0090\n",
      "ID: 258, True: 1, Pred: 1, Prob: 0.8267\n",
      "ID: 259, True: 1, Pred: 1, Prob: 0.8057\n",
      "ID: 260, True: 0, Pred: 1, Prob: 0.5391\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  59%|█████▉    | 10/17 [00:16<00:11,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 288, True: 0, Pred: 0, Prob: 0.0478\n",
      "ID: 289, True: 1, Pred: 1, Prob: 0.6226\n",
      "ID: 290, True: 1, Pred: 0, Prob: 0.3040\n",
      "ID: 291, True: 1, Pred: 1, Prob: 0.6704\n",
      "ID: 292, True: 1, Pred: 0, Prob: 0.2310\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|██████▍   | 11/17 [00:18<00:09,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 320, True: 0, Pred: 0, Prob: 0.0019\n",
      "ID: 321, True: 1, Pred: 1, Prob: 0.9175\n",
      "ID: 322, True: 0, Pred: 0, Prob: 0.2751\n",
      "ID: 323, True: 1, Pred: 1, Prob: 0.8975\n",
      "ID: 324, True: 0, Pred: 0, Prob: 0.2480\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|███████   | 12/17 [00:19<00:07,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 352, True: 0, Pred: 0, Prob: 0.1755\n",
      "ID: 353, True: 1, Pred: 1, Prob: 0.5156\n",
      "ID: 354, True: 0, Pred: 0, Prob: 0.0100\n",
      "ID: 355, True: 0, Pred: 1, Prob: 0.7090\n",
      "ID: 356, True: 1, Pred: 0, Prob: 0.0002\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  76%|███████▋  | 13/17 [00:21<00:06,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 384, True: 1, Pred: 1, Prob: 0.8066\n",
      "ID: 385, True: 0, Pred: 0, Prob: 0.0099\n",
      "ID: 386, True: 1, Pred: 1, Prob: 0.9805\n",
      "ID: 387, True: 0, Pred: 1, Prob: 0.9185\n",
      "ID: 388, True: 0, Pred: 0, Prob: 0.2878\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  82%|████████▏ | 14/17 [00:22<00:04,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 416, True: 1, Pred: 0, Prob: 0.4148\n",
      "ID: 417, True: 1, Pred: 1, Prob: 0.8740\n",
      "ID: 418, True: 0, Pred: 0, Prob: 0.4961\n",
      "ID: 419, True: 1, Pred: 1, Prob: 0.5176\n",
      "ID: 420, True: 1, Pred: 1, Prob: 0.8428\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|████████▊ | 15/17 [00:24<00:03,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 448, True: 1, Pred: 1, Prob: 0.9707\n",
      "ID: 449, True: 0, Pred: 0, Prob: 0.4456\n",
      "ID: 450, True: 1, Pred: 1, Prob: 0.9497\n",
      "ID: 451, True: 1, Pred: 0, Prob: 0.1461\n",
      "ID: 452, True: 0, Pred: 1, Prob: 0.6582\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  94%|█████████▍| 16/17 [00:26<00:01,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 480, True: 0, Pred: 0, Prob: 0.0804\n",
      "ID: 481, True: 1, Pred: 1, Prob: 0.8481\n",
      "ID: 482, True: 1, Pred: 1, Prob: 0.9658\n",
      "ID: 483, True: 0, Pred: 0, Prob: 0.3040\n",
      "ID: 484, True: 1, Pred: 1, Prob: 0.8198\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 17/17 [00:26<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processed. Sample predictions:\n",
      "ID: 512, True: 1, Pred: 0, Prob: 0.4980\n",
      "ID: 513, True: 0, Pred: 0, Prob: 0.0237\n",
      "ID: 514, True: 0, Pred: 0, Prob: 0.2338\n",
      "ID: 515, True: 1, Pred: 0, Prob: 0.1060\n",
      "ID: 516, True: 0, Pred: 0, Prob: 0.0888\n",
      "--------------------------------------------------\n",
      "Predictions completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    with open(predictions_file, mode='a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # softmax获取概率分布\n",
    "            probs = torch.softmax(logits, dim=-1).cpu()\n",
    "            preds = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "            prob_positive = probs[:, 1].cpu().numpy()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(prob_positive)\n",
    "            \n",
    "            if 'id' in test_data.column_names:\n",
    "                ids = batch['id'].cpu().numpy()\n",
    "                texts = test_data['loan_data'][batch['input_ids'].indices]\n",
    "            else:\n",
    "                # 使用 DataLoader 的索引\n",
    "                start_idx = len(all_labels) - len(labels)\n",
    "                ids = list(range(start_idx, start_idx + len(labels)))\n",
    "                texts = test_data['loan_data'][start_idx : start_idx + len(labels)]\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                writer.writerow([\n",
    "                    ids[i],\n",
    "                    texts[i],\n",
    "                    labels[i].item(),\n",
    "                    preds[i].item(),\n",
    "                    prob_positive[i]\n",
    "                ])\n",
    "            \n",
    "            print(f\"Batch processed. Sample predictions:\")\n",
    "            for i in range(min(5, len(labels))):\n",
    "                print(f\"ID: {ids[i]}, True: {labels[i].item()}, Pred: {preds[i].item()}, Prob: {prob_positive[i]:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "print(\"Predictions completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd8bc3f-fe69-4717-a9e9-943179702f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7181\n",
      "Precision: 0.7165\n",
      "Recall (Sensitivity): 0.7220\n",
      "F1 Score: 0.7192\n",
      "Specificity: 0.7143\n",
      "AUC: 0.7647\n",
      "Metrics saved to 'results/metrics.txt'.\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "result_dir = 'results'\n",
    "metrics_file_path = os.path.join(result_dir, \"metrics.txt\")\n",
    "with open(metrics_file_path, mode='w', encoding='utf-8') as f:\n",
    "    f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "    f.write(f\"Recall (Sensitivity): {recall:.4f}\\n\")\n",
    "    f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "    f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "    f.write(f\"AUC: {auc:.4f}\\n\")\n",
    "\n",
    "print(f\"Metrics saved to '{metrics_file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcc5751-c336-46ea-a483-4b42aa54c406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
