{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from unsloth import FastLanguageModel\n",
    "import wandb\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0aed2a-22b1-4ecb-8a94-f21cd68fd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 贷款数据特征变量权重\n",
    "feature_importance = {\n",
    "    \"Loan amount\": 1.2,\n",
    "    \"DTI\": 1.5,\n",
    "    \"Employment Title\": 0.8,\n",
    "    \"Employment Length\": 1.0,\n",
    "    \"Home Ownership\": 1.1,\n",
    "    \"Annual Income\": 1.6,\n",
    "    \"Verification Status\": 1.0,\n",
    "    \"Grade\": 2.0,\n",
    "    \"Purpose\": 0.9,\n",
    "    \"Description\": 0.7,\n",
    "    \"Title\": 0.8,\n",
    "    \"Open Accounts\": 1.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da3f905-e5e2-483d-a704-004b66a97771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将重要性列表转换为一个对应的tensor\n",
    "importance_tensor = torch.tensor([v for v in feature_importance.values()], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f393160-0e92-4a27-9e88-29239c4cc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义自定义加权交叉熵损失函数\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, importance_tensor):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.importance_tensor = importance_tensor\n",
    "\n",
    "    def forward(self, predictions, targets, features):\n",
    "        # 确保 features 维度与 importance_tensor 匹配\n",
    "        if features.shape[1] != len(self.importance_tensor):\n",
    "            raise ValueError(f\"Feature dimension {features.shape[1]} does not match importance tensor length {len(self.importance_tensor)}\")\n",
    "\n",
    "        # 确保 importance_tensor 和 features 在同一设备上\n",
    "        self.importance_tensor = self.importance_tensor.to(features.device)\n",
    "\n",
    "        # 如果 predictions 是 [batch_size, seq_len, num_classes]，需要展平为 [batch_size * seq_len, num_classes]\n",
    "        if predictions.dim() == 3:\n",
    "            batch_size, seq_len, num_classes = predictions.size()\n",
    "            predictions = predictions.view(-1, num_classes)  # 展平为 [batch_size * seq_len, num_classes]\n",
    "\n",
    "        # 如果 targets 是 [batch_size, seq_len]，需要展平为 [batch_size * seq_len]\n",
    "        if targets.dim() == 1:\n",
    "            # 假设每个样本在序列中有相同的标签，重复 targets 以匹配序列长度\n",
    "            targets = targets.unsqueeze(1).expand(-1, seq_len).contiguous()\n",
    "        \n",
    "        # 确保 targets 被展平为 [batch_size * seq_len]\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # 确保 predictions 和 targets 的 batch_size 一致\n",
    "        if predictions.size(0) != targets.size(0):\n",
    "            raise ValueError(f\"Expected predictions and targets to have the same batch_size after flattening, but got {predictions.size(0)} and {targets.size(0)}\")\n",
    "\n",
    "        # 计算标准交叉熵损失\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss()(predictions, targets)\n",
    "\n",
    "        # 将特征张量展平并乘以权重\n",
    "        weighted_features = features.view(features.size(0), -1) * self.importance_tensor.unsqueeze(0)\n",
    "\n",
    "        # 使用加权特征对整体损失进行加权\n",
    "        weighted_loss = cross_entropy_loss * torch.mean(weighted_features)\n",
    "\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48183d76-1f18-48ad-a89b-12ff2074563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_path = \"example\"\n",
    "dataset = load_dataset(loan_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7066c9f2-f9fb-40cb-a709-b34e4c61e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 167153\n",
      "})\n",
      "train_data[0]: {'text': 'Loan amount: 13000, DTI: 20.05, Employment Title: DARCARS Toyota, Employment Length: 10+ years, Home Ownership: RENT, Annual Income: 55000.0, Verification Status: Not Verified, Grade: B-B3, Purpose: debt_consolidation, Description:   Pay off all my credit cards<br/>, Title: Credit Card, Open Accounts: 16', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "print(\"train_data:\", train_data)\n",
    "print(\"train_data[0]:\", train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a01e10-f6ef-471a-b2b2-5f789d2ec610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance data sample\n",
    "label_1_data = [data for data in train_data if data['label'] == 1]\n",
    "label_0_data = [data for data in train_data if data['label'] == 0]\n",
    "\n",
    "num_label_1 = len(label_1_data)\n",
    "balanced_label_0_data = random.sample(label_0_data, num_label_1)\n",
    "balanced_data = label_1_data + balanced_label_0_data\n",
    "\n",
    "# random data layout\n",
    "random.shuffle(balanced_data)\n",
    "\n",
    "dataset = Dataset.from_list(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3572ac04-6e97-4955-9abf-f0faefe53b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 167153\n",
      "Balanced dataset size: 1920\n",
      "Number of label 1 samples: 960\n",
      "Number of label 0 samples: 960\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original dataset size: {len(train_data)}\")\n",
    "print(f\"Balanced dataset size: {len(balanced_data)}\")\n",
    "print(f\"Number of label 1 samples: {len(label_1_data)}\")\n",
    "print(f\"Number of label 0 samples: {len(balanced_label_0_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85fc96f-db5e-404a-adaa-63bcab4b525a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f81e94b9f1a4637a4329468cba8c11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'loan_data'],\n",
      "    num_rows: 1920\n",
      "})\n",
      "{'label': 0, 'loan_data': 'Loan amount: 25000, DTI: 32.96, Employment Title: Teacher, Employment Length: 10+ years, Home Ownership: RENT, Annual Income: 75000.0, Verification Status: Source Verified, Grade: B-B5, Purpose: debt_consolidation, Description: nan, Title: Debt consolidation, Open Accounts: 12'}\n"
     ]
    }
   ],
   "source": [
    "def rename_columns(example):\n",
    "    example[\"loan_data\"] = example.pop(\"text\")\n",
    "    example[\"label\"] = example.pop(\"label\")\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(rename_columns)\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90edcd60-c5fd-4051-8354-54751d7dcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # True to use 4bit quantization / reduce memory usage (for T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4cf3f3-6169-4ddf-af49-4bb19ec2f23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A800-SXM4-80GB. Max memory: 79.325 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292c8cef75b0494facf700b3b6097ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model/Mistral-7B-Instruct-v0.3 does not have a padding token! Will use pad_token = [control_768].\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/mistral-7b-v0.2-bnb-4bit\",\n",
    "    model_name = \"model/Mistral-7B-Instruct-v0.3\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a27128-ba6c-440e-86e5-bd2c65ae993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2024.8 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # can improve fine-tuning, at attention/feed fwd layers\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = 16, # more change to pre-train weights but care overfitting\n",
    "    lora_dropout = 0.05, # any, but 0 if perf opti.\n",
    "    bias = \"none\",    # any, but \"none\" is perf  opti.\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 11,\n",
    "    use_rslora = False,  # support rank stabilized LoRA\n",
    "    loftq_config = None, # LoftQ support\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1674b9-a396-4ac8-ba9e-30f7692d7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Preparation\n",
    "prompt = \"\"\"You are a highly intelligent and detailed artificial intelligence assistant with a deep understanding of financial data, specifically in predicting loan defaults.\n",
    "Your task is to accurately classify loan data into one of two possible outcomes:\n",
    "- 0: The loan is fully paid off (no default).\n",
    "- 1: The loan has defaulted (borrower failed to meet the repayment terms).\n",
    "\n",
    "The input data will provide various details about the loan and the borrower's financial situation. Your goal is to carefully analyze this information and determine the loan's status based on the provided features.\n",
    "\n",
    "You are expected to generate a response that is one of the following labels:\n",
    "- 0: The loan is fully paid off.\n",
    "- 1: The loan has defaulted.\n",
    "\n",
    "Your classification must be precise and match the best possible outcome for the given loan data.\n",
    "\n",
    "Here is the loan data you need to classify:\n",
    "### Loan Data:\n",
    "{loan_data}\n",
    "### Loan Status:\n",
    "{loan_status}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50295f2f-1d5c-4653-819a-3493201ecd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add EOS special token, according to previously loaded tokenizer\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def format_prompts(examples):\n",
    "    inputs = examples[\"loan_data\"]\n",
    "    outputs = examples[\"label\"]\n",
    "    texts = []\n",
    "    for inp, output in zip(inputs, outputs):\n",
    "        # Add end of string token to prevent infinite generations.\n",
    "        text = prompt.format(loan_data=inp, loan_status=output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\":texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9affaca-9604-4dc0-b24a-306d05d6e2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98aa08e35347482a823a57e07eb310fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: Dataset({\n",
      "    features: ['label', 'loan_data', 'text'],\n",
      "    num_rows: 1920\n",
      "})\n",
      "You are a highly intelligent and detailed artificial intelligence assistant with a deep understanding of financial data, specifically in predicting loan defaults.\n",
      "Your task is to accurately classify loan data into one of two possible outcomes:\n",
      "- 0: The loan is fully paid off (no default).\n",
      "- 1: The loan has defaulted (borrower failed to meet the repayment terms).\n",
      "\n",
      "The input data will provide various details about the loan and the borrower's financial situation. Your goal is to carefully analyze this information and determine the loan's status based on the provided features.\n",
      "\n",
      "You are expected to generate a response that is one of the following labels:\n",
      "- 0: The loan is fully paid off.\n",
      "- 1: The loan has defaulted.\n",
      "\n",
      "Your classification must be precise and match the best possible outcome for the given loan data.\n",
      "\n",
      "Here is the loan data you need to classify:\n",
      "### Loan Data:\n",
      "Loan amount: 25000, DTI: 32.96, Employment Title: Teacher, Employment Length: 10+ years, Home Ownership: RENT, Annual Income: 75000.0, Verification Status: Source Verified, Grade: B-B5, Purpose: debt_consolidation, Description: nan, Title: Debt consolidation, Open Accounts: 12\n",
      "### Loan Status:\n",
      "0</s>\n"
     ]
    }
   ],
   "source": [
    "# Building prompts\n",
    "dataset= dataset.map(format_prompts, batched = True)\n",
    "\n",
    "# Print a sample :\n",
    "print(\"dataset:\", dataset)\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e21a9860-4810-47df-bc38-f28cb51bfe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义Trainer以使用自定义损失函数\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, dataset_text_field=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dataset_text_field = dataset_text_field  # 添加 dataset_text_field 字段\n",
    "        self.train_dataset = self.train_dataset.map(self.tokenize_function, batched=True)\n",
    "\n",
    "        self.train_dataset = self.train_dataset.remove_columns([\"loan_data\", \"text\"])\n",
    "        \n",
    "    def tokenize_function(self, examples):\n",
    "        # 使用 tokenizer 对 text 进行编码，生成 input_ids 和 attention_mask\n",
    "        return self.tokenizer(examples[self.dataset_text_field], padding=\"max_length\", truncation=True)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # 如果有自定义的 dataset_text_field 使用它\n",
    "        # print(\"inputs:\", inputs)\n",
    "        inputs = {key: val.to(model.device) for key, val in inputs.items()}  # 将 inputs 移动到模型所在的设备\n",
    "\n",
    "        # 获取模型的输出\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        logits = outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "\n",
    "        # 获取特征（假设特征在 inputs[\"features\"]）\n",
    "        features = inputs[\"features\"] if \"features\" in inputs else torch.ones((logits.shape[0], len(feature_importance))).to(logits.device)\n",
    "\n",
    "        # 调用自定义损失函数\n",
    "        loss = WeightedCrossEntropyLoss(importance_tensor)(logits, labels, features)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "700d03ad-4910-469d-8212-23de906e3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 1,\n",
    "    # max_steps = 110,\n",
    "    learning_rate = 2e-4, # 2e-5\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 11,\n",
    "    output_dir = \"outputs/mistral-7b-instruct-v0.3-0910\",\n",
    "    run_name = \"mistral-7b-instruct-v0.3\",\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps = 1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit = 2,\n",
    "    report_to = \"wandb\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24b5a79d-f8bb-4998-aa5e-08a190bd7ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Parameter 'function'=<bound method CustomTrainer.tokenize_function of <__main__.CustomTrainer object at 0x7f4a0c1371c0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460f63d66edb4aadbec6cc7ed75ab2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.dataset: Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1920\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# init the trainer\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    args = training_args\n",
    ")\n",
    "print(\"trainer.dataset:\", trainer.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc73b8e-92ff-4e6e-b05c-56e99ad2b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,920 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 240\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunjie_chiang\u001b[0m (\u001b[33mjjchiang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/hpc2hdd/home/simonsyguo/junjiejiang/algorithm/FinLLM_loan_pred/wandb/run-20240910_205925-oajd2sdy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jjchiang/huggingface/runs/oajd2sdy' target=\"_blank\">mistral-7b-instruct-v0.3</a></strong> to <a href='https://wandb.ai/jjchiang/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jjchiang/huggingface' target=\"_blank\">https://wandb.ai/jjchiang/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jjchiang/huggingface/runs/oajd2sdy' target=\"_blank\">https://wandb.ai/jjchiang/huggingface/runs/oajd2sdy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/240 07:56 < 7:44:39, 0.01 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>14.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.718900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''train'''\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune_jjj/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:368\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''train'''\n",
    "trainer = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
